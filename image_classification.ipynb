{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramesh-dev-code/openvino-projects/blob/main/image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291dc37b",
      "metadata": {
        "id": "291dc37b"
      },
      "source": [
        "# Hello Image Classification\n",
        "\n",
        "This basic introduction to OpenVINO™ shows how to do inference with an image classification models such as renset-50 (Tensorflow) and resnet-18 (PyTorch).\n",
        "\n",
        "Refer to [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) to download a wide range of pre-trained models and ready-to-run demos. For more information about how OpenVINO IR models are created, refer to the [TensorFlow to OpenVINO](../tensorflow-classification-to-openvino/tensorflow-classification-to-openvino.ipynb) tutorial.\n",
        "\n",
        "\n",
        "#### Table of contents:\n",
        "\n",
        "- [Imports](#Imports)\n",
        "- [Download the Model and data samples](#Download-the-Model-and-data-samples)\n",
        "- [Select inference device](#Select-inference-device)\n",
        "- [Load the Model](#Load-the-Model)\n",
        "- [Load an Image](#Load-an-Image)\n",
        "- [Do Inference](#Do-Inference)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "246482f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246482f5",
        "outputId": "bca533ae-e3ed-4f52-f0a5-f52a6e68682d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "# Install openvino package\n",
        "%pip install -q \"openvino>=2023.1.0\" opencv-python tqdm\n",
        "\n",
        "if platform.system() != \"Windows\":\n",
        "    %pip install -q \"matplotlib>=3.4\"\n",
        "else:\n",
        "    %pip install -q \"matplotlib>=3.4,<3.7\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c8cbe5",
      "metadata": {
        "id": "e4c8cbe5"
      },
      "source": [
        "## Imports\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "41ee9436",
      "metadata": {
        "id": "41ee9436"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import openvino as ov\n",
        "\n",
        "# Fetch `notebook_utils` module\n",
        "import requests\n",
        "\n",
        "r = requests.get(\n",
        "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
        ")\n",
        "\n",
        "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
        "\n",
        "from notebook_utils import download_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4afb34",
      "metadata": {
        "id": "fb4afb34"
      },
      "source": [
        "## Download the Model and data samples\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16bf5919-78d0-47e9-8d8c-98e1fdbb7910",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16bf5919-78d0-47e9-8d8c-98e1fdbb7910",
        "outputId": "68438eb2-f601-46df-d228-409786db2a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: omz_downloader: command not found\n"
          ]
        }
      ],
      "source": [
        "!omz_downloader --name resnet-50-tf -o ./artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "beeb9d58-a7f0-470a-a33f-8488829ab34f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beeb9d58-a7f0-470a-a33f-8488829ab34f",
        "outputId": "a9b13403-85fe-4daa-b985-263d90811a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: omz_converter: command not found\n"
          ]
        }
      ],
      "source": [
        "! omz_converter --name resnet-50-tf -o ./artifacts -d ./artifacts --precision FP16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2cf255-ac39-4656-b742-ec12520f423b",
      "metadata": {
        "id": "5d2cf255-ac39-4656-b742-ec12520f423b"
      },
      "source": [
        "## Select inference device\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "select device from dropdown list for running inference using OpenVINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "66cc2ae5-bbb4-49c7-892b-bfd23343e580",
      "metadata": {
        "tags": [
          "hide-input"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e6e26c19ef654d279c4a65984f1cdb6c",
            "5e6f33f7bee7477093e0df7e7fe8e73b",
            "3f0c745cfb764806a486ad03a8e61901"
          ]
        },
        "id": "66cc2ae5-bbb4-49c7-892b-bfd23343e580",
        "outputId": "e36c1a12-c642-4301-ca70-76f3b5c85ac7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6e26c19ef654d279c4a65984f1cdb6c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "core = ov.Core()\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value=\"AUTO\",\n",
        "    description=\"Device:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e49ae7",
      "metadata": {
        "id": "55e49ae7"
      },
      "source": [
        "## Load the Model\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e3c4d6fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "e3c4d6fc",
        "outputId": "0c0213eb-71b1-4747-d696-9796f0fb082a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Exception from src/inference/src/cpp/core.cpp:90:\nCheck 'util::directory_exists(path) || util::file_exists(path)' failed at src/frontends/common/src/frontend.cpp:113:\nFrontEnd API failed with GeneralFailure:\nir: Could not open the file: \"artifacts/public/resnet-50-tf/FP16/resnet-50-tf.xml\"\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7264b5380593>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_xml_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_artifacts_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_xml_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcompiled_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mread_model\u001b[0;34m(self, model, weights)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     def compile_model(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/cpp/core.cpp:90:\nCheck 'util::directory_exists(path) || util::file_exists(path)' failed at src/frontends/common/src/frontend.cpp:113:\nFrontEnd API failed with GeneralFailure:\nir: Could not open the file: \"artifacts/public/resnet-50-tf/FP16/resnet-50-tf.xml\"\n\n"
          ]
        }
      ],
      "source": [
        "base_artifacts_dir = Path(\"./artifacts/public/resnet-50-tf/FP16/\").expanduser()\n",
        "file_name = \"resnet-50-tf.xml\"\n",
        "model_xml_path = base_artifacts_dir / file_name\n",
        "core = ov.Core()\n",
        "model = core.read_model(model=model_xml_path)\n",
        "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
        "\n",
        "output_layer = compiled_model.output(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19fc080",
      "metadata": {
        "id": "a19fc080"
      },
      "source": [
        "## Load an Image\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca45b68",
      "metadata": {
        "id": "eca45b68"
      },
      "outputs": [],
      "source": [
        "# Download the image from the openvino_notebooks storage\n",
        "data_dir = Path(\"./data\").expanduser()\n",
        "image_filename = data_dir / \"t3.jpg\"\n",
        "# The MobileNet model expects images in RGB format.\n",
        "image = cv2.cvtColor(cv2.imread(filename=str(image_filename)), code=cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Resize to MobileNet image shape\n",
        "input_image = cv2.resize(src=image, dsize=(224, 224))\n",
        "#input_image = input_image.transpose(2, 0, 1)\n",
        "\n",
        "# Reshape to model input shape.\n",
        "input_image = np.expand_dims(input_image, 0)\n",
        "plt.imshow(image);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be327b6",
      "metadata": {
        "id": "6be327b6"
      },
      "source": [
        "## Do Inference\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed78a71",
      "metadata": {
        "id": "1ed78a71"
      },
      "outputs": [],
      "source": [
        "result_infer = compiled_model([input_image])[output_layer]\n",
        "result_index = np.argmax(result_infer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c3c3fa",
      "metadata": {
        "id": "e0c3c3fa"
      },
      "outputs": [],
      "source": [
        "imagenet_filename = download_file(\n",
        "    \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/datasets/imagenet/imagenet_2012.txt\",\n",
        "    directory=\"data\",\n",
        ")\n",
        "\n",
        "imagenet_classes = imagenet_filename.read_text().splitlines()\n",
        "# The model description states that for this model, class 0 is a background.\n",
        "# Therefore, a background must be added at the beginning of imagenet_classes.\n",
        "imagenet_classes = [\"background\"] + imagenet_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf29578c",
      "metadata": {
        "tags": [],
        "id": "bf29578c"
      },
      "outputs": [],
      "source": [
        "imagenet_classes[result_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e78d6a-0194-4f8f-9a25-2b44c2a283eb",
      "metadata": {
        "id": "45e78d6a-0194-4f8f-9a25-2b44c2a283eb"
      },
      "source": [
        "## Conversion of PyTorch Model using OpenVINO model conversion API  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c64f441-fc3b-43e7-9100-a9d66af878f0",
      "metadata": {
        "id": "8c64f441-fc3b-43e7-9100-a9d66af878f0"
      },
      "outputs": [],
      "source": [
        "import openvino as ov\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import torchvision\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# get default weights using available weights Enum for model\n",
        "weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
        "\n",
        "# create model topology and load weights\n",
        "model = resnet18(weights=weights)\n",
        "\n",
        "# switch model to inference mode\n",
        "model.eval()\n",
        "\n",
        "# Convert model to openvino.runtime.Model object\n",
        "ov_model = ov.convert_model(model, input=[[1, 3, 224, 224]])\n",
        "prep = ov.preprocess.PrePostProcessor(ov_model)\n",
        "prep.input(\"x\").tensor().set_layout(ov.Layout(\"nchw\"))\n",
        "prep.input(\"x\").preprocess().mean([123.675,116.28,103.53])\n",
        "prep.input(\"x\").preprocess().scale([58.395,57.12,57.375])\n",
        "ov_model = prep.build()\n",
        "base_artifacts_dir = Path(\"./artifacts/public/resnet-18-pytorch/ov_format/\").expanduser()\n",
        "ov_model_path = base_artifacts_dir / \"resnet-18-pytorch.xml\"\n",
        "ov.save_model(ov_model,ov_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f29767b-3a38-4859-9f67-4833e5e2849b",
      "metadata": {
        "id": "9f29767b-3a38-4859-9f67-4833e5e2849b"
      },
      "outputs": [],
      "source": [
        "ov_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfde924a-9caa-4942-af4d-339645829fdb",
      "metadata": {
        "id": "bfde924a-9caa-4942-af4d-339645829fdb"
      },
      "source": [
        "### Execute inference with PyTorch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c06c90-2be4-4774-a156-72c677a49da5",
      "metadata": {
        "id": "a2c06c90-2be4-4774-a156-72c677a49da5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import torchvision\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# get default weights using available weights Enum for model\n",
        "weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
        "\n",
        "# create model topology and load weights\n",
        "model = resnet18(weights=weights)\n",
        "\n",
        "# switch model to inference mode\n",
        "model.eval()\n",
        "\n",
        "# Prepare Input Data\n",
        "data_dir = Path(\"./data\").expanduser()\n",
        "image_filename = data_dir / \"goldfish.jpg\"\n",
        "# The model expects images in RGB format.\n",
        "image = Image.open(image_filename)\n",
        "\n",
        "# Initialize the Weight Transforms\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Apply it to the input image\n",
        "img_transformed = preprocess(image)\n",
        "\n",
        "# Add batch dimension to image tensor\n",
        "input_tensor = img_transformed.unsqueeze(0)\n",
        "\n",
        "imagenet_filename = data_dir / \"imagenet_2012.txt\"\n",
        "\n",
        "# Perform model inference on input tensor\n",
        "pt_infer_result = model(input_tensor)\n",
        "result_index = np.argmax(pt_infer_result.detach().numpy())\n",
        "imagenet_classes = imagenet_filename.read_text().splitlines()\n",
        "imagenet_classes[result_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8961daf6-cf09-472d-b68b-2f0ea8428498",
      "metadata": {
        "id": "8961daf6-cf09-472d-b68b-2f0ea8428498"
      },
      "source": [
        "### Execute inference with the optimized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae0a4af-080c-48e4-b6ee-5cc537bdd169",
      "metadata": {
        "id": "aae0a4af-080c-48e4-b6ee-5cc537bdd169"
      },
      "outputs": [],
      "source": [
        "import openvino as ov\n",
        "import cv2\n",
        "import numpy as np\n",
        "core = ov.Core()\n",
        "base_artifacts_dir = Path(\"./artifacts/public/resnet-18-pytorch/ov_format/\").expanduser()\n",
        "ov_model_path = base_artifacts_dir / \"resnet-18-pytorch.xml\"\n",
        "model_ov = core.read_model(model=ov_model_path)\n",
        "compiled_model = core.compile_model(model=model_ov, device_name=\"CPU\")\n",
        "output_layer = compiled_model.output(0)\n",
        "\n",
        "# Image Preprocessing\n",
        "image = cv2.cvtColor(cv2.imread(str(image_filename)), cv2.COLOR_BGR2RGB)\n",
        "input_image = cv2.resize(src=image, dsize=(224, 224))\n",
        "input_image = input_image.transpose(2, 0, 1)\n",
        "input_tensor_ov = np.expand_dims(input_image, 0)\n",
        "\n",
        "# Display the preprocessed input image\n",
        "#cv2.imshow(\"Input\",image)\n",
        "#cv2.waitKey(0)\n",
        "\n",
        "# Execute inference on the compiled model\n",
        "result_infer = compiled_model([input_tensor_ov])[output_layer]\n",
        "result_index = np.argmax(result_infer)\n",
        "\n",
        "# Get the predicted class from the ImageNet 2012 label file\n",
        "imagenet_filename = data_dir / \"imagenet_2012.txt\"\n",
        "imagenet_classes = imagenet_filename.read_text().splitlines()\n",
        "print('Inference Result: ', imagenet_classes[result_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "998654bd-d2e2-43c3-a5f9-5261a58c9b01",
      "metadata": {
        "id": "998654bd-d2e2-43c3-a5f9-5261a58c9b01"
      },
      "source": [
        "### Benchmark PyTorch Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750c4071-af34-4794-84d4-3ee1786d3830",
      "metadata": {
        "id": "750c4071-af34-4794-84d4-3ee1786d3830"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "# Run model inference\n",
        "model(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6627222d-e717-4b5a-b963-d751baf710e2",
      "metadata": {
        "id": "6627222d-e717-4b5a-b963-d751baf710e2"
      },
      "source": [
        "### Benchmark OpenVINO Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2120df-8ae0-4bfa-8160-400d3cb79b4a",
      "metadata": {
        "id": "5b2120df-8ae0-4bfa-8160-400d3cb79b4a"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "# Run model inference\n",
        "compiled_model([input_tensor_ov])[output_layer]"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "cd5437b16fd5f67deabdf7e7132d444cc39310b7a33353e3b68dab8f7e829ac5"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "openvino_notebooks": {
      "imageUrl": "https://user-images.githubusercontent.com/36741649/127172572-1cdab941-df5f-42e2-a367-2b334a3db6d8.jpg",
      "tags": {
        "categories": [
          "First Steps"
        ],
        "libraries": [],
        "other": [],
        "tasks": [
          "Image Classification"
        ]
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6e26c19ef654d279c4a65984f1cdb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_5e6f33f7bee7477093e0df7e7fe8e73b",
            "style": "IPY_MODEL_3f0c745cfb764806a486ad03a8e61901"
          }
        },
        "5e6f33f7bee7477093e0df7e7fe8e73b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0c745cfb764806a486ad03a8e61901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}